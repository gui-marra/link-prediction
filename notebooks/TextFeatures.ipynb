{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vtex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextTreatment(text):\n",
    "    \n",
    "    text = RemoveLowerCase(text)\n",
    "    text = RemovePonctuation(text)\n",
    "    text = RemoveUnderlines(text)\n",
    "    text = RemoveStopWords(text)\n",
    "    text = Lemmatize(text, FrenchStemmer())\n",
    "    \n",
    "    # Things to consider: rare words removal \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveLowerCase(text):\n",
    "    return ' '.join([x.lower() for x in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemovePonctuation (text):\n",
    "    return re.sub('\\W+',' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveUnderlines (text):\n",
    "    return re.sub('_','', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWords(text):\n",
    "    stop = stopwords.words('french', 'english')\n",
    "    return ' '.join([x for x in text.split(' ') if x not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmatize(text, stemmer):\n",
    "    return ' '.join([stemmer.stem(w) for w in text.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/raw/node_information/text'\n",
    "no_nodes = 33226\n",
    "\n",
    "files = []\n",
    "for i in range(no_nodes):\n",
    "    files.append(os.path.join(path, str(i) + '.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file number 1000\n",
      "Reading file number 2000\n",
      "Reading file number 3000\n",
      "Reading file number 4000\n",
      "Reading file number 5000\n",
      "Reading file number 6000\n",
      "Reading file number 7000\n",
      "Reading file number 8000\n",
      "Reading file number 9000\n",
      "Reading file number 10000\n",
      "Reading file number 11000\n",
      "Reading file number 12000\n",
      "Reading file number 13000\n",
      "Reading file number 14000\n",
      "Reading file number 15000\n",
      "Reading file number 16000\n",
      "Reading file number 17000\n",
      "Reading file number 18000\n",
      "Reading file number 19000\n",
      "Reading file number 20000\n",
      "Reading file number 21000\n",
      "Reading file number 22000\n",
      "Reading file number 23000\n",
      "Reading file number 24000\n",
      "Reading file number 25000\n",
      "Reading file number 26000\n",
      "Reading file number 27000\n",
      "Reading file number 28000\n",
      "Reading file number 29000\n",
      "Reading file number 30000\n",
      "Reading file number 31000\n",
      "Reading file number 32000\n",
      "Reading file number 33000\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for i in range(len(files)):\n",
    "    \n",
    "    if i>0 and i%1000 == 0:\n",
    "        print('Reading file number ' + str(i))\n",
    "    \n",
    "    f = open(files[i], \"r\", errors = 'ignore')\n",
    "    text = f.read()\n",
    "    text = TextTreatment(text)\n",
    "    documents.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Vocabulary\n",
    "vocabulary = set()\n",
    "for i, doc in enumerate(documents):\n",
    "    vocabulary.update(doc.split(' '))\n",
    "    \n",
    "    if i>0 and i%1000 == 0:\n",
    "        print('Reading document number ' + str(i))\n",
    "    \n",
    "    \n",
    "vocabulary = list(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intializating the tfIdf model\n",
    "tfidf = TfidfVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "# Fit the TfIdf model\n",
    "tfidf.fit(documents)\n",
    "\n",
    "# Transform the TfIdf model\n",
    "tfidf_tran=tfidf.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save sparse matrix for future use\n",
    "scipy.sparse.save_npz(\"../results/models/tf-idf_matrix\", tfidf_tran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "tagged_data = [TaggedDocument(doc, [i]) for i, doc in enumerate(documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 30\n",
    "vec_size = 100\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1,\n",
    "                workers=12)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch+1))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"../results/models/doc2vec/vec100.model\")\n",
    "print(\"Model Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
