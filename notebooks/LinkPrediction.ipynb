{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %conda install numpy \n",
    "# %conda install pandas\n",
    "# %conda install -c anaconda scikit-learn \n",
    "# %conda install networkx\n",
    "# %conda install -c conda-forge node2vec \n",
    "# %conda install -c conda-forge tpot\n",
    "# %conda install -c conda-forge ipywidgets\n",
    "# %conda install pytorch torchvision -c pytorch\n",
    "# %conda install -c anaconda cudatoolkit=10.0\n",
    "# %pip install xgboost (GPU not supported in conda environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook consists of an explorative analysis of the problem of predicting links in web pages. Input data:  \n",
    "* pairs of pages (two nodes in a graph) and a boolean variable indicating if there is a link (an edge) between them.  \n",
    "* text of all pages\n",
    "\n",
    "This way the strategy to approach this problem is first to extract as much relevant information as possible from the inputs, that is, to engineer features for the graph and for the text of the pages, and then to train and tune a classification model.  The problem is thus divided:\n",
    "\n",
    "##### Feature Engineering\n",
    "1. Networkx Link Prediction Features\n",
    "2. Node embedding Features\n",
    "3. Text Features\n",
    "\n",
    "##### Classification Models\n",
    "4. Miscellaneous Classifiers\n",
    "5. XGBoost\n",
    "\n",
    "##### Prediction\n",
    "6. Submission prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Initial Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv('../data/raw/training.txt', header = None, sep = ' ', names = ['node', 'target', 'edge'])\n",
    "X = links[['node', 'target']]\n",
    "y = links['edge']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Networkx Link Prediction Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "1.1 Split the dataset <br/>\n",
    "1.2 Create graph connections with the training set <br/>\n",
    "1.3 Predict new coefficients for training and test set (feature generation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateGraph (X, y, directed = False):\n",
    "    \n",
    "    if directed:\n",
    "        G = nx.from_pandas_edgelist(X, 'node', 'target', create_using=nx.DiGraph())\n",
    "    else:    \n",
    "        G = nx.from_pandas_edgelist(X[y == 1], 'node', 'target', create_using=nx.Graph())\n",
    "        \n",
    "    G.add_nodes_from(range(33226))\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AppendNextworkxFeature (function, G, X):\n",
    "    \n",
    "    column_name = str(function).split()[1]\n",
    "    \n",
    "    tuple_list = list(X[['node','target']].itertuples(index=False, name=None))\n",
    "    coef_generator = function(G, tuple_list)\n",
    "    coef_df = pd.DataFrame(coef_generator)\n",
    "    X.insert(2, column_name, list(coef_df[2]), allow_duplicates = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nxGenerateFeatures (X_train, X_test, y_train):\n",
    "    \n",
    "    # 1.2 Create Graph\n",
    "    # Total number of nodes=pages: 33.226\n",
    "    G = CreateGraph (X_train, y_train)\n",
    "    \n",
    "    # 1.3 Predict new coefficient/feature for defined link prediction function\n",
    "    linkPredictionFunctions = [nx.resource_allocation_index, nx.jaccard_coefficient, nx.adamic_adar_index, nx.preferential_attachment]\n",
    "    for function in linkPredictionFunctions:\n",
    "        AppendNextworkxFeature(function, G, X_train)\n",
    "        AppendNextworkxFeature(function, G, X_test)\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveData (path, X_train, X_test, y_train, y_test=[]):\n",
    "    \n",
    "    parent_folder = '/'.join([x for x in path.split('/')[:-2]])\n",
    "    \n",
    "    X_train.to_csv(os.path.join(path, 'X_train.csv'), sep=',', index=False)\n",
    "    X_test.to_csv(os.path.join(path, 'X_test.csv'), sep=',', index=False)\n",
    "    \n",
    "    pd.DataFrame(y_train).to_csv(os.path.join(parent_folder,'y_train.csv'), sep=',', index=False, header=False)\n",
    "    \n",
    "    if len(y_test) > 0:\n",
    "        pd.DataFrame(y_test).to_csv(os.path.join(parent_folder,'y_test.csv'), sep=',', index=False, header=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData (path):\n",
    "    \n",
    "    parent_folder = '/'.join([x for x in path.split('/')[:-2]])\n",
    "    \n",
    "    X_train = pd.read_csv(os.path.join(path, 'X_train.csv'), sep=',')\n",
    "    X_test = pd.read_csv(os.path.join(path, 'X_test.csv'), sep=',')\n",
    "    y_train = np.genfromtxt(os.path.join(parent_folder,'y_train.csv'), delimiter=',', skip_header=0)\n",
    "    \n",
    "    try:\n",
    "        y_test = np.genfromtxt(os.path.join(parent_folder,'y_test.csv'), delimiter=',', skip_header=0)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    except Exception:\n",
    "        pass\n",
    "        \n",
    "    return X_train, X_test, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Stratified Split \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 and 1.3 Generate features\n",
    "X_train, X_test = nxGenerateFeatures (X_train, X_test, y_train)\n",
    "\n",
    "# Save Networkx Intermediate Results\n",
    "SaveData('../data/intermediate/networkx/', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Networkx Intermediate Results\n",
    "X_train, X_test, y_train, y_test = LoadData('../data/intermediate/networkx/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Node embedding features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "2.1. Fit node2vec model to the graph, and embed it's edges <br/>\n",
    "2.2. Apply the model to train and test set (feature generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2VecGenerateFeatures(X_train, X_test, y_train, savePath = None, loadPath = None, workers = 1):\n",
    "    \n",
    "    DG = CreateGraph(X_train, y_train, directed = True)\n",
    "\n",
    "    # 2.1 Fit or Load model\n",
    "    if loadPath:\n",
    "        model = Word2Vec.load(loadPath)\n",
    "    else:\n",
    "        node2vec = Node2Vec(DG, dimensions=20, walk_length=16, num_walks=100, workers=workers)\n",
    "        model = node2vec.fit(window=4, min_count=1)\n",
    "        if savePath:\n",
    "            model.save(savePath)\n",
    "\n",
    "            \n",
    "    # Embed edges using Hadamard Embedder\n",
    "    edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n",
    "\n",
    "\n",
    "    # 2.2 Apply embedding to each element\n",
    "    emb_train = [ edges_embs[(str(i), str(j))] for i,j in zip(X_train['node'], X_train['target'])]\n",
    "    emb_test = [ edges_embs[(str(i), str(j))] for i,j in zip(X_test['node'], X_test['target'])]\n",
    "    \n",
    "    X_train['edge_norm'] = [LA.norm(v) for v in emb_train]\n",
    "    X_test['edge_norm'] = [LA.norm(v) for v in emb_test]\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_train, X_test = node2VecGenerateFeatures(X_train, X_test, y_train, loadPath = '../results/models/node2vec/emb1_train.model')\n",
    "except Exception:\n",
    "    X_train, X_test = node2VecGenerateFeatures(X_train, X_test, y_train, savePath = '../results/models/node2vec/emb1_train.model', workers = 12)\n",
    "    pass\n",
    "\n",
    "# Save Node2Vec Intermediate Results\n",
    "SaveData('../data/intermediate/node2vec/', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Node2Vec Intermediate Data\n",
    "X_train, X_test, y_train, y_test = LoadData('../data/intermediate/node2vec/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Load doc2vec embedding model trained in 'TextFeatures' notebook  <br/>\n",
    "2.2. Apply the model to train and test set (feature generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization Options: TF-IDF or doc2vec \n",
    "# Format Options: concatenating nodes or cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "def doc2vecGenerateFeatures (X_train, X_test, loadPath, difference = False):\n",
    "    \n",
    "    # 3.1 Load Model \n",
    "    model = Doc2Vec.load(loadPath) \n",
    "    \n",
    "    # 3.2 Apply embedding to each element\n",
    "    if difference:\n",
    "        vec_train = [ model.infer_vector([str(i)]) -  model.infer_vector([str(j)]) for i, j in zip(X_train['node'], X_train['target']) ]\n",
    "        vec_test = [ model.infer_vector([str(i)]) -  model.infer_vector([str(j)]) for i, j in zip(X_test['node'], X_test['target']) ]\n",
    "        \n",
    "    else:\n",
    "        vec_train = [ model.infer_vector([str(i), str(j)]) for i, j in zip(X_train['node'], X_train['target']) ]\n",
    "        vec_test = [ model.infer_vector([str(i), str(j)]) for i, j in zip(X_test['node'], X_test['target']) ]\n",
    "    \n",
    "    X_train = X_train.join(pd.DataFrame(vec_train))\n",
    "    X_test = X_test.join(pd.DataFrame(vec_test))\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = doc2vecGenerateFeatures(X_train, X_test, loadPath=\"../results/models/doc2vec/vec10.model\", difference=True)\n",
    "\n",
    "# Save Node2Vec Intermediate Results\n",
    "SaveData('../data/intermediate/doc2vec/', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Node2Vec Intermediate Data\n",
    "X_train, X_test, y_train, y_test = LoadData('../data/intermediate/doc2vec/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Miscellaneous Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scale(X):\n",
    "    scaled_features = StandardScaler().fit_transform(X.values)\n",
    "    return pd.DataFrame(scaled_features, index = X.index, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation parameters\n",
    "'''Better performance withou scaling'''\n",
    "scale = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "if 'node' and 'target' in X_train.columns:\n",
    "    X_train = X_train.drop(columns = ['node', 'target'])\n",
    "if 'node' and 'target' in X_test.columns:\n",
    "    X_test = X_test.drop(columns = ['node', 'target'])\n",
    "    \n",
    "\n",
    "if scale:  \n",
    "    X_train = Scale(X_train)\n",
    "    X_test = Scale(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "##Classifiers\n",
    "from sklearn.ensemble import AdaBoostClassifier #begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted\n",
    "from sklearn.ensemble import BaggingClassifier #Bagging classifier fits base classifiers each on random subsets of the original dataset and aggregate their individual predictions\n",
    "from sklearn.ensemble import ExtraTreesClassifier #Extremely Random Trees: This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting\n",
    "from sklearn.ensemble import GradientBoostingClassifier #GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier #Classifier implementing the k-nearest neighbors vote.\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for classifiers:\n",
      "AdaBoostClassifier\n",
      "Train scores: \n",
      "f1-score: 0.912\tAccuracy: 0.891\tPrecision: 0.915\tRecall: 0.909\tROC-AUC: 0.885\n",
      "Test scores: \n",
      "f1-score: 0.899\tAccuracy: 0.876\tPrecision: 0.913\tRecall: 0.886\tROC-AUC: 0.872\n",
      "\n",
      "**********************************************************************\n",
      "BaggingClassifier\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d62a2b405c17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/ssd/anaconda3/envs/link/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \"\"\"\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/ssd/anaconda3/envs/link/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    370\u001b[0m                                \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             delayed(_parallel_build_estimators)(\n",
      "\u001b[0;32m/mnt/disks/ssd/anaconda3/envs/link/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/ssd/anaconda3/envs/link/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/ssd/anaconda3/envs/link/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/ssd/anaconda3/envs/link/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/ssd/anaconda3/envs/link/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/ssd/anaconda3/envs/link/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/ssd/anaconda3/envs/link/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/ssd/anaconda3/envs/link/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\u001b[0m in \u001b[0;36m_parallel_build_estimators\u001b[0;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnot_indices_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/ssd/anaconda3/envs/link/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \"\"\"\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/ssd/anaconda3/envs/link/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Classification Models\n",
    "print(\"Summary for classifiers:\")\n",
    "\n",
    "clf = [\n",
    "            [AdaBoostClassifier(), \"AdaBoostClassifier\"],\n",
    "            [BaggingClassifier(), \"BaggingClassifier\"],\n",
    "            [ExtraTreesClassifier(), \"ExtraTreesClassifier\"],\n",
    "            [GradientBoostingClassifier(), \"GradientBoostClassifier\"],\n",
    "            [DecisionTreeClassifier(), \"DecisionTreeClassifier\"],\n",
    "            [RandomForestClassifier(), \"RandomForestClassifier\"]\n",
    "        ]\n",
    "\n",
    "performance_train = {}\n",
    "performance_test = {}\n",
    "    \n",
    "for classifier, clf_name in clf: performance_train[clf_name] = []\n",
    "for classifier, clf_name in clf: performance_test[clf_name] = []\n",
    "\n",
    "    \n",
    "for elem in clf: #Use each classifier in clf\n",
    "    classifier = elem[0]\n",
    "    classifier_name = elem[1]\n",
    "    print(classifier_name)\n",
    "        \n",
    "    try:    \n",
    "        classifier.fit(X_train, y_train)\n",
    "            \n",
    "        y_hat = classifier.predict(X_train)\n",
    "        #Train Scores:\n",
    "        f1_train = f1_score(y_train, y_hat)\n",
    "        accuracy_train = accuracy_score(y_train, y_hat)\n",
    "        precision_train = precision_score(y_train, y_hat)\n",
    "        recall_train = recall_score(y_train, y_hat)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_hat)\n",
    "        #Print train Scores\n",
    "        print(f\"Train scores: \\nf1-score: {round(f1_train,3)}\\tAccuracy: {round(accuracy_train, 3)}\\tPrecision: {round(precision_train,3)}\\tRecall: {round(recall_train,3)}\\tROC-AUC: {round(roc_auc_train,3)}\")\n",
    "        #Sava train scors for comparison\n",
    "        performance_train[classifier_name].append(f1_train)\n",
    "        performance_train[classifier_name].append(accuracy_train)\n",
    "        performance_train[classifier_name].append(precision_train)\n",
    "        performance_train[classifier_name].append(recall_train)\n",
    "        performance_train[classifier_name].append(roc_auc_train)\n",
    "           \n",
    "        y_pred = classifier.predict(X_test)\n",
    "        #Test scores\n",
    "        f1_test = f1_score(y_test, y_pred)\n",
    "        accuracy_test = accuracy_score(y_test, y_pred)\n",
    "        precision_test = precision_score(y_test, y_pred)\n",
    "        recall_test = recall_score(y_test, y_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_pred)\n",
    "        #Print test scores          \n",
    "        print(f\"Test scores: \\nf1-score: {round(f1_test,3)}\\tAccuracy: {round(accuracy_test,3)}\\tPrecision: {round(precision_test,3)}\\tRecall: {round(recall_test,3)}\\tROC-AUC: {round(roc_auc_test,3)}\")\n",
    "          #Save test scores\n",
    "        performance_test[classifier_name].append(f1_test)\n",
    "        performance_test[classifier_name].append(accuracy_test)\n",
    "        performance_test[classifier_name].append(precision_test)\n",
    "        performance_test[classifier_name].append(recall_test)\n",
    "        performance_test[classifier_name].append(roc_auc_test)\n",
    "\n",
    "        print(\"\\n**********************************************************************\")\n",
    "    except ImportError:\n",
    "        print(\"Classifier \\\"\" + classifier_name + \"failed.\")\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tpot auto-ml tool for hyper-parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def tpot (X_train, y_train, X_test = None, y_test = None,\n",
    "          export_file = '../results/models/tpot/exported_pipeline.py', n_jobs = 1):\n",
    "    \n",
    "    if 'node' and 'target' in X_train.columns:\n",
    "        X_train = X_train.drop(columns = ['node', 'target'])\n",
    "    if 'node' and 'target' in X_test.columns:\n",
    "        X_test = X_test.drop(columns = ['node', 'target'])\n",
    "\n",
    "    tpot = TPOTClassifier(generations = 5, population_size = 40, cv=3, verbosity=2, scoring = 'f1', n_jobs=6)\n",
    "\n",
    "    tpot.fit(X_train, y_train)\n",
    "    tpot.export(export_file)\n",
    "    print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) networkx\n",
    "tpot(X_train, y_train, X_test, y_test, export_file = '../results/models/tpot/nx_exported_pipeline.py',  n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2) node2vec\n",
    "tpot(X_train, y_train, X_test, y_test, export_file = '../results/models/tpot/n2v_exported_pipeline.py',  n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) doc2vec\n",
    "tpot(X_train, y_train, X_test, y_test, export_file = '../results/models/tpot/d2v_exported_pipeline.py',  n_jobs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def XGB(LR = 0.1, n_est = 1000, max_d = 5, min_c = 1, gm = 0, colsample = 0.8, subs = 1, lambd = 0, alpha = 0):\n",
    "   \n",
    "    xgb_model = XGBClassifier( learning_rate=LR, \n",
    "                                            n_estimators=n_est,\n",
    "                                            max_depth=max_d,\n",
    "                                            min_child_weight=min_c,                         \n",
    "                                            gamma=min_c,\n",
    "                                            colsample_bytree=colsample,\n",
    "                                            subsample=subs,\n",
    "                                            objective ='binary:logistic',                   \n",
    "                                            reg_lambda=lambd,\n",
    "                                            reg_alpha=alpha,\n",
    "                                            scale_pos_weight = 1,\n",
    "                                            seed=42)\n",
    "\n",
    "\n",
    "    xgb_model.fit(X_train,y_train)\n",
    "    y_pred_train = xgb_model.predict(X_train)\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "\n",
    "    print('performance over the training set: ' + str(f1_score(y_train, y_pred_train)))\n",
    "    print('performance over the test set: ' + str(f1_score(y_test, y_pred)) + '\\n')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    \n",
    "def XGB_tuning (LR = 0.1, n_est = 1000, max_d = 5, min_c = 1, gm = 0, colsample = 0.8, subs = 1,\n",
    "                lambd = 0, alpha = 0, param_test = {'learning_rate':[i/100.0 for i in range(5,20,2)]} ):\n",
    "    \n",
    "    gsearch = GridSearchCV(estimator = XGBClassifier( learning_rate=LR, \n",
    "                                                    n_estimators=n_est,\n",
    "                                                    max_depth=max_d,\n",
    "                                                    min_child_weight=min_c,                         \n",
    "                                                    gamma=min_c,\n",
    "                                                    colsample_bytree=colsample,\n",
    "                                                    subsample = subs,\n",
    "                                                    objective ='binary:logistic',\n",
    "                                                    reg_lambda=lambd,\n",
    "                                                    reg_alpha=alpha,\n",
    "                                                    scale_pos_weight = 1,\n",
    "                                                    seed=42), \n",
    "                            param_grid = param_test, \n",
    "                            scoring='f1',\n",
    "                            n_jobs=4, \n",
    "                            cv=3)\n",
    "\n",
    "\n",
    "    gsearch.fit(X_train, y_train)\n",
    "    return gsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance over the training set: 0.9218679460737016\n",
      "performance over the test set: 0.9007730122542035\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.85      0.84     34008\n",
      "         1.0       0.91      0.89      0.90     56752\n",
      "\n",
      "    accuracy                           0.88     90760\n",
      "   macro avg       0.87      0.87      0.87     90760\n",
      "weighted avg       0.88      0.88      0.88     90760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 1 - First XGB - A little bit overfitted, but could improve more the training set\n",
    "LearningRate = 0.1\n",
    "n_estimators = 1000\n",
    "\n",
    "# Fix Learning Rate and n_estimators\n",
    "XGB(LearningRate, n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'min_child_weight': 5} CV_Score: 0.9138881713274564\n",
      "performance over the training set: 0.9145638801610834\n",
      "performance over the test set: 0.9011386840486147\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.86      0.84     34008\n",
      "         1.0       0.91      0.89      0.90     56752\n",
      "\n",
      "    accuracy                           0.88     90760\n",
      "   macro avg       0.87      0.87      0.87     90760\n",
      "weighted avg       0.88      0.88      0.88     90760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 2 - Tuning max_depth and min_child_weight\n",
    "\n",
    "parameters_test = {\n",
    "    'max_depth':range(3,7,2),\n",
    "    'min_child_weight':range(1,7,2)\n",
    "}\n",
    "\n",
    "gsearch2 = XGB_tuning (LR = LearningRate, n_est = n_estimators, param_test=parameters_test)\n",
    "best_max_depth, best_min_child_weight = gsearch2.best_params_['max_depth'], gsearch2.best_params_['min_child_weight']\n",
    "print(f\"{gsearch2.best_params_} CV_Score: {gsearch2.best_score_}\")\n",
    "\n",
    "# Evaluation\n",
    "XGB(LR = LearningRate, n_est = n_estimators, max_d=best_max_depth, min_c= best_min_child_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.0} CV_Score: 0.9139307459555347\n",
      "performance over the training set: 0.9145638801610834\n",
      "performance over the test set: 0.9011386840486147\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.86      0.84     34008\n",
      "         1.0       0.91      0.89      0.90     56752\n",
      "\n",
      "    accuracy                           0.88     90760\n",
      "   macro avg       0.87      0.87      0.87     90760\n",
      "weighted avg       0.88      0.88      0.88     90760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 3 - Tuning Gamma \n",
    "parameters_test = { \n",
    "    'gamma':[i/10.0 for i in range(0,5)] \n",
    "}\n",
    "\n",
    "gsearch3 = XGB_tuning (LR = LearningRate, n_est = n_estimators, max_d=best_max_depth, min_c= best_min_child_weight,\n",
    "                      param_test=parameters_test)\n",
    "best_gamma = gsearch3.best_params_['gamma']\n",
    "print(f\"{gsearch3.best_params_} CV_Score: {gsearch3.best_score_}\")\n",
    "\n",
    "# Evaluation\n",
    "XGB(LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, min_c = best_min_child_weight, gm = best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.9, 'subsample': 0.8} CV_Score: 0.9140435480710711\n",
      "performance over the training set: 0.9162213415295286\n",
      "performance over the test set: 0.9010383706938812\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.86      0.84     34008\n",
      "         1.0       0.91      0.89      0.90     56752\n",
      "\n",
      "    accuracy                           0.88     90760\n",
      "   macro avg       0.87      0.87      0.87     90760\n",
      "weighted avg       0.88      0.88      0.88     90760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 4 - Tuning colsample_bytree and subsaample\n",
    "parameters_test = {\n",
    "    'colsample_bytree':[i/10.0 for i in range(7,11)],\n",
    "    'subsample':[i/10.0 for i in range(7,11)]\n",
    "}\n",
    "\n",
    "gsearch4 = XGB_tuning (LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, min_c = best_min_child_weight,\n",
    "                      gm = best_gamma, param_test=parameters_test)\n",
    "best_colsample_bytree, best_subsample = gsearch4.best_params_['colsample_bytree'], gsearch4.best_params_['subsample']\n",
    "print(f\"{gsearch4.best_params_} CV_Score: {gsearch4.best_score_}\")\n",
    "\n",
    "# Evaluation\n",
    "XGB(LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, \n",
    "                       min_c = best_min_child_weight, gm = best_gamma,  colsample = best_colsample_bytree, \n",
    "                       subs = best_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5 - Tuning Regularization Parameters\n",
    "# Lambda L2 Regularization\n",
    "# Ampha L1 Regularization\n",
    "\n",
    "parameters_test = {\n",
    "    'reg_lambda':[1e-2, 0.1, 0.5, 1, 2, 10],\n",
    "    'reg_alpha':[1e-2, 0.1, 0.5, 1, 2, 10],\n",
    "}\n",
    "\n",
    "gsearch5 = XGB_tuning (LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, \n",
    "                       min_c = best_min_child_weight, gm = best_gamma,  colsample = best_colsample_bytree, \n",
    "                       subs = best_subsample, param_test=parameters_test)\n",
    "\n",
    "best_reg_lambda, best_reg_alpha = gsearch5.best_params_['reg_lambda'], gsearch5.best_params_['reg_alpha']\n",
    "print(f\"{gsearch5.best_params_} CV_Score: {gsearch5.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closer Look\n",
    "parameters_test = {\n",
    "    'reg_lambda':[best_reg_lambda*0.8, best_reg_lambda, best_reg_lambda*1.2, best_reg_lambda*1.5],\n",
    "    'reg_alpha':[best_reg_alpha*0.8, best_reg_alpha, best_reg_alpha*1.2, best_reg_alpha*1.5]\n",
    "}\n",
    "\n",
    "gsearch5 = XGB_tuning (LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, min_c = best_min_child_weight,\n",
    "                       gm = best_gamma,  colsample = best_colsample_bytree, subs = best_subsample, \n",
    "                       param_test=parameters_test)\n",
    "\n",
    "best_reg_lambda, best_red_alpha = gsearch5.best_params_['reg_lambda'], gsearch5.best_params_['reg_alpha']\n",
    "print(f\"{gsearch5.best_params_} CV_Score: {gsearch5.best_score_}\")\n",
    "\n",
    "# Evaluation\n",
    "XGB(LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, min_c = best_min_child_weight,\n",
    "                       gm = best_gamma,  colsample = best_colsample_bytree, subs = best_subsample, \n",
    "                       lambd = best_reg_lambda, alpha = best_reg_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # STEP 6 - Reducing Learning Rate and Adding More Trees\n",
    "n_estimators *= 5\n",
    "parameters_test = {\n",
    "    'learning_rate':[i/100.0 for i in range(2,10)]\n",
    "}\n",
    "\n",
    "gsearch6 = XGB_tuning (LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, min_c = best_min_child_weight,\n",
    "                       gm = best_gamma,  colsample = best_colsample_bytree, subs = best_subsample, \n",
    "                       lambd = best_reg_lambda, alpha = best_reg_alpha, param_test=parameters_test)\n",
    "\n",
    "LearningRate = gsearch6.best_params_['learning_rate']\n",
    "print(f\"{gsearch6.best_params_} CV_Score: {gsearch6.best_score_}\")\n",
    "\n",
    "\n",
    "# Final Evaluation\n",
    "XGB(LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, min_c = best_min_child_weight,\n",
    "                       gm = best_gamma,  colsample = best_colsample_bytree, subs = best_subsample, \n",
    "                       lambd = best_reg_lambda, alpha = best_reg_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Submission Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Testing File\n",
    "X_submission = pd.read_csv('../data/raw/testing.txt', header = None, sep = ' ', names = ['node', 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networkx Feature Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Networkx Features\n",
    "X, X_submission = nxGenerateFeatures(X, X_submission, y)\n",
    "\n",
    "# Save Processed Results\n",
    "SaveData('../data/processed/networkx/', X, X_submission, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Networkx Processed Data\n",
    "X, X_submission, y = LoadData ('../data/processed/networkx/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Node2Vec Feature Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Node2Vec Features\n",
    "try:\n",
    "    X, X_submission = node2VecGenerateFeatures(X, X_submission, y, loadPath = '../results/models/node2vec/emb1.model')\n",
    "except Exception:\n",
    "    X, X_submission = node2VecGenerateFeatures(X, X_submission, y, savePath = '../results/models/node2vec/emb1.model', workers = 12)\n",
    "    pass\n",
    "\n",
    "# Save Processed Results\n",
    "SaveData('../data/processed/node2vec/', X, X_submission, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Node2Vec Processed Data\n",
    "X, X_submission, y = LoadData ('../data/processed/node2vec/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Doc2Vec Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "###  Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "# could be 'or' since these columns are always removed together\n",
    "if 'node' and 'target' in X.columns:\n",
    "    X = X.drop(columns = ['node', 'target'])\n",
    "    \n",
    "if 'node' and 'target' in X_submission.columns:\n",
    "    X_submission = X_submission.drop(columns = ['node', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "result_file_name = '../results/predictions/predictions.csv'\n",
    "pd.DataFrame(y_pred, columns = ['predicted']).to_csv(result_file_name, sep=',', index=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Results\n",
    "submission = pd.read_csv(result_file_name, sep=',')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### REFERENCES ####\n",
    "# https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
