{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pip install xgboost # (GPU supported)\n",
    "%conda install numpy \n",
    "%conda install pandas\n",
    "%conda install -c anaconda scikit-learn \n",
    "%conda install networkx\n",
    "%conda install -c conda-forge node2vec \n",
    "%conda install -c conda-forge tpot\n",
    "%conda install -c conda-forge ipywidgets\n",
    "%conda install pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "import file_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook consists of an explorative analysis of the problem of predicting links in web pages. Input data:  \n",
    "* pairs of pages (two nodes in a graph) and a boolean variable indicating if there is a link (an edge) between them.  \n",
    "* text of all pages\n",
    "\n",
    "This way the strategy to approach this problem is first to extract as much relevant information as possible from the inputs, that is, to engineer features for the graph and for the text of the pages, and then to train and tune a classification model.  The problem is thus divided:\n",
    "\n",
    "##### Feature Engineering\n",
    "1. Networkx Link Prediction features\n",
    "2. TF-IDF text features\n",
    "3. Doc2Vec embedding features\n",
    "\n",
    "\n",
    "##### Classification Models\n",
    "4. Miscellaneous Classifiers\n",
    "5. XGBoost\n",
    "\n",
    "##### Prediction\n",
    "6. Submission prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Initial Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv('../data/raw/training.txt', header = None, sep = ' ', names = ['node', 'target', 'edge'])\n",
    "X = links[['node', 'target']]\n",
    "y = links['edge']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Networkx Link Prediction Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "1.1 Split the dataset <br/>\n",
    "1.2 Create graph connections with the training set <br/>\n",
    "1.3 Predict new coefficients for training and test set (feature generation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateGraph (X, y, directed = False):\n",
    "    \n",
    "    if directed:\n",
    "        G = nx.from_pandas_edgelist(X, 'node', 'target', create_using=nx.DiGraph())\n",
    "    else:    \n",
    "        G = nx.from_pandas_edgelist(X[y == 1], 'node', 'target', create_using=nx.Graph())\n",
    "        \n",
    "    G.add_nodes_from(range(33226))\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AppendNextworkxFeature (function, G, X):\n",
    "    \n",
    "    column_name = str(function).split()[1]\n",
    "    \n",
    "    tuple_list = list(X[['node','target']].itertuples(index=False, name=None))\n",
    "    coef_generator = function(G, tuple_list)\n",
    "    coef_df = pd.DataFrame(coef_generator)\n",
    "    X.insert(2, column_name, list(coef_df[2]), allow_duplicates = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nxGenerateFeatures (X_train, X_test, y_train):\n",
    "    \n",
    "    # 1.2 Create Graph\n",
    "    # Total number of nodes=pages: 33.226\n",
    "    G = CreateGraph (X_train, y_train)\n",
    "    \n",
    "    # 1.3 Predict new coefficient/feature for defined link prediction function\n",
    "    linkPredictionFunctions = [nx.resource_allocation_index, nx.jaccard_coefficient, nx.adamic_adar_index, nx.preferential_attachment]\n",
    "    for function in linkPredictionFunctions:\n",
    "        AppendNextworkxFeature(function, G, X_train)\n",
    "        AppendNextworkxFeature(function, G, X_test)\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Stratified Split \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 and 1.3 Generate features\n",
    "X_train, X_test = nxGenerateFeatures (X_train, X_test, y_train)\n",
    "\n",
    "# Save Networkx Intermediate Results\n",
    "file_io.SaveData('../data/intermediate/networkx/', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Networkx Intermediate Results\n",
    "X_train, X_test, y_train, y_test = file_io.LoadData('../data/intermediate/networkx/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>target</th>\n",
       "      <th>preferential_attachment</th>\n",
       "      <th>adamic_adar_index</th>\n",
       "      <th>jaccard_coefficient</th>\n",
       "      <th>resource_allocation_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9154</td>\n",
       "      <td>9156</td>\n",
       "      <td>176</td>\n",
       "      <td>0.276938</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.027027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29190</td>\n",
       "      <td>8765</td>\n",
       "      <td>7596</td>\n",
       "      <td>0.291149</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>0.002704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3239</td>\n",
       "      <td>16566</td>\n",
       "      <td>468</td>\n",
       "      <td>0.402430</td>\n",
       "      <td>0.006329</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1294</td>\n",
       "      <td>3248</td>\n",
       "      <td>1666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21519</td>\n",
       "      <td>32895</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363032</th>\n",
       "      <td>1297</td>\n",
       "      <td>326</td>\n",
       "      <td>540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363033</th>\n",
       "      <td>4688</td>\n",
       "      <td>24489</td>\n",
       "      <td>549</td>\n",
       "      <td>0.264754</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.001621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363034</th>\n",
       "      <td>6839</td>\n",
       "      <td>15644</td>\n",
       "      <td>21</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363035</th>\n",
       "      <td>2344</td>\n",
       "      <td>12969</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363036</th>\n",
       "      <td>7706</td>\n",
       "      <td>4918</td>\n",
       "      <td>54205</td>\n",
       "      <td>1.918590</td>\n",
       "      <td>0.006702</td>\n",
       "      <td>0.066247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363037 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         node  target  preferential_attachment  adamic_adar_index  \\\n",
       "0        9154    9156                      176           0.276938   \n",
       "1       29190    8765                     7596           0.291149   \n",
       "2        3239   16566                      468           0.402430   \n",
       "3        1294    3248                     1666           0.000000   \n",
       "4       21519   32895                        1           0.000000   \n",
       "...       ...     ...                      ...                ...   \n",
       "363032   1297     326                      540           0.000000   \n",
       "363033   4688   24489                      549           0.264754   \n",
       "363034   6839   15644                       21           0.000000   \n",
       "363035   2344   12969                       35           0.000000   \n",
       "363036   7706    4918                    54205           1.918590   \n",
       "\n",
       "        jaccard_coefficient  resource_allocation_index  \n",
       "0                  0.038462                   0.027027  \n",
       "1                  0.003110                   0.002704  \n",
       "2                  0.006329                   0.083333  \n",
       "3                  0.000000                   0.000000  \n",
       "4                  0.000000                   0.000000  \n",
       "...                     ...                        ...  \n",
       "363032             0.000000                   0.000000  \n",
       "363033             0.029412                   0.001621  \n",
       "363034             0.000000                   0.000000  \n",
       "363035             0.000000                   0.000000  \n",
       "363036             0.006702                   0.066247  \n",
       "\n",
       "[363037 rows x 6 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Truncated SVD on tf-idf embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveTruncatedSVD(sparseMatrixPath = \"../results/models/tf-idf/emb_matrix.npz\"):\n",
    "    \n",
    "    try:\n",
    "        tf_idf = scipy.sparse.load_npz(sparseMatrixPath)  \n",
    "    except Exception:\n",
    "        print(\"TF-IDF model not saved, please run TextFeatures.ipynb\")\n",
    "        raise\n",
    "        \n",
    "    svd = TruncatedSVD(n_components=100, n_iter=100, random_state=42)\n",
    "    svd.fit(tf_idf)\n",
    "\n",
    "    svd_matrix = svd.transform(tf_idf)\n",
    "    pd.DataFrame(svd_matrix).to_csv(\"../results/models/tf-idf/svd.csv\", sep=',', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveTruncatedSVD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as CS\n",
    "import similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendCosineSimilarity (X_train, X_test, embedding, column_name = ''):\n",
    "    \n",
    "    cosine_similarity_train = [ CS(embedding[i].reshape(1, -1), embedding[j].reshape(1, -1))[0][0] for i, j in zip(X_train['node'], X_train['target']) ]\n",
    "    cosine_similarity_test  = [ CS(embedding[i].reshape(1, -1), embedding[j].reshape(1, -1))[0][0] for i, j in zip(X_test['node'], X_test['target']) ]\n",
    "    \n",
    "    X_train[column_name] = cosine_similarity_train\n",
    "    X_test[column_name] = cosine_similarity_test\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendTS_SS(X_train, X_test, embedding, column_name = '', mult_factor = 1):\n",
    "    \n",
    "    ts_ss = similarity.TS_SS()\n",
    "    ts_score_train = [ ts_ss(embedding[i], embedding[j])*mult_factor for i, j in zip(X_train['node'], X_train['target']) ]\n",
    "    ts_score_test = [ ts_ss(embedding[i], embedding[j])*mult_factor for i, j in zip(X_test['node'], X_test['target']) ]\n",
    "    \n",
    "    X_train[column_name] = ts_score_train\n",
    "    X_test[column_name] = ts_score_test\n",
    "    \n",
    "    X_train = X_train.fillna(value=0)\n",
    "    X_test = X_test.fillna(value=0)\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension with most variance by a large amount (17%)\n",
    "def appendDimensionZero(X_train, X_test, embedding, column_name = ''):\n",
    "    \n",
    "    dim0_train = [ [embedding[i][0], embedding[j][0]] for i, j in zip(X_train['node'], X_train['target']) ]\n",
    "    dim0_test = [ [embedding[i][0], embedding[j][0]] for i, j in zip(X_test['node'], X_test['target']) ]\n",
    "    \n",
    "    X_train = X_train.join(pd.DataFrame(dim0_train)).rename(columns={0: column_name+'dim0_node', 1: column_name+'dim0_target'})\n",
    "    X_test = X_test.join(pd.DataFrame(dim0_test)).rename(columns={0: column_name+'dim0_node', 1: column_name+'dim0_target'})\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfGenerateFeatures (X_train, X_test):\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        tf_idf = scipy.sparse.load_npz(\"../results/models/tf-idf/emb_matrix.npz\")  \n",
    "    except Exception:\n",
    "        print(\"TF-IDF model not saved, please run TextFeatures.ipynb\")\n",
    "        raise\n",
    "    \n",
    "    X_train, X_test = appendCosineSimilarity(X_train, X_test, embedding=tf_idf, column_name='cosine_sim')\n",
    "    \n",
    "    print('generated cosine similarity')\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        svd_matrix = np.genfromtxt(\"../results/models/tf-idf/svd.csv\", delimiter=',')\n",
    "    except Exception:\n",
    "        saveTruncatedSVD()\n",
    "        svd_matrix = np.genfromtxt(\"../results/models/tf-idf/svd.csv\", delimiter=',')\n",
    "        pass\n",
    "    \n",
    "    X_train, X_test = appendTS_SS(X_train, X_test, embedding=svd_matrix, column_name='ts-ss_sim', mult_factor = 1e5)\n",
    "    print('generated ts ss')\n",
    "    X_train, X_test = appendDimensionZero(X_train, X_test, embedding=svd_matrix)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = tfidfGenerateFeatures(X_train, X_test)\n",
    "\n",
    "# Save tf-idf Intermediate Results\n",
    "file_io.SaveData('../data/intermediate/tf-idf/', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tf-idf Intermediate Data\n",
    "X_train, X_test, y_train, y_test = file_io.LoadData('../data/intermediate/tf-idf/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Doc2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Load doc2vec embedding model trained in 'TextFeatures' notebook  <br/>\n",
    "2.2. Apply the model to train and test set (feature generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vecGenerateFeatures (X_train, X_test, loadPath):\n",
    "    \n",
    "    # 3.1 Load Model \n",
    "    model = Doc2Vec.load(loadPath) \n",
    "    \n",
    "    # 3.2 Apply embedding to each element\n",
    "    ts_ss = similarity.TS_SS()\n",
    "    vec_train = [ ts_ss( model.infer_vector([str(i)]), model.infer_vector([str(j)]) ) for i, j in zip(X_train['node'], X_train['target']) ]\n",
    "    vec_test = [ ts_ss( model.infer_vector([str(i)]), model.infer_vector([str(j)]) ) for i, j in zip(X_test['node'], X_test['target']) ]\n",
    "        \n",
    "    \n",
    "    X_train = X_train.join(pd.DataFrame(vec_train))\n",
    "    X_test = X_test.join(pd.DataFrame(vec_test))\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = doc2vecGenerateFeatures(X_train, X_test, loadPath=\"../results/models/doc2vec/vec100.model\")\n",
    "\n",
    "# Save Doc2Vec Intermediate Results\n",
    "file_io.SaveData('../data/intermediate/doc2vec/', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Doc2Vec Intermediate Data\n",
    "X_train, X_test, y_train, y_test = file_io.LoadData('../data/intermediate/doc2vec/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Miscellaneous Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scale(X):\n",
    "    scaled_features = StandardScaler().fit_transform(X.values)\n",
    "    return pd.DataFrame(scaled_features, index = X.index, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "'''After some tests I realized that keeping node and target index number increases performance by a little amount'''\n",
    "# if 'node' and 'target' in X_train.columns:\n",
    "#     X_train = X_train.drop(columns = ['node', 'target'])\n",
    "# if 'node' and 'target' in X_test.columns:\n",
    "#     X_test = X_test.drop(columns = ['node', 'target'])\n",
    "    \n",
    "\n",
    "'''Better performance withou scaling'''\n",
    "scale = False \n",
    "if scale:  \n",
    "    X_train = Scale(X_train)\n",
    "    X_test = Scale(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "##Classifiers\n",
    "from sklearn.ensemble import AdaBoostClassifier #begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted\n",
    "from sklearn.ensemble import BaggingClassifier #Bagging classifier fits base classifiers each on random subsets of the original dataset and aggregate their individual predictions\n",
    "from sklearn.ensemble import ExtraTreesClassifier #Extremely Random Trees: This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting\n",
    "from sklearn.ensemble import GradientBoostingClassifier #GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier #Classifier implementing the k-nearest neighbors vote.\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classification Models\n",
    "print(\"Summary for classifiers:\")\n",
    "\n",
    "clf = [\n",
    "            [AdaBoostClassifier(), \"AdaBoostClassifier\"],\n",
    "            [BaggingClassifier(), \"BaggingClassifier\"],\n",
    "            [ExtraTreesClassifier(), \"ExtraTreesClassifier\"],\n",
    "            [GradientBoostingClassifier(), \"GradientBoostClassifier\"],\n",
    "            [DecisionTreeClassifier(), \"DecisionTreeClassifier\"],\n",
    "            [RandomForestClassifier(), \"RandomForestClassifier\"]\n",
    "        ]\n",
    "\n",
    "performance_train = {}\n",
    "performance_test = {}\n",
    "    \n",
    "for classifier, clf_name in clf: performance_train[clf_name] = []\n",
    "for classifier, clf_name in clf: performance_test[clf_name] = []\n",
    "\n",
    "    \n",
    "for elem in clf: #Use each classifier in clf\n",
    "    classifier = elem[0]\n",
    "    classifier_name = elem[1]\n",
    "    print(classifier_name)\n",
    "        \n",
    "    try:    \n",
    "        classifier.fit(X_train, y_train)\n",
    "            \n",
    "        y_hat = classifier.predict(X_train)\n",
    "        #Train Scores:\n",
    "        f1_train = f1_score(y_train, y_hat)\n",
    "        accuracy_train = accuracy_score(y_train, y_hat)\n",
    "        precision_train = precision_score(y_train, y_hat)\n",
    "        recall_train = recall_score(y_train, y_hat)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_hat)\n",
    "        #Print train Scores\n",
    "        print(f\"Train scores: \\nf1-score: {round(f1_train,3)}\\tAccuracy: {round(accuracy_train, 3)}\\tPrecision: {round(precision_train,3)}\\tRecall: {round(recall_train,3)}\\tROC-AUC: {round(roc_auc_train,3)}\")\n",
    "        #Sava train scors for comparison\n",
    "        performance_train[classifier_name].append(f1_train)\n",
    "        performance_train[classifier_name].append(accuracy_train)\n",
    "        performance_train[classifier_name].append(precision_train)\n",
    "        performance_train[classifier_name].append(recall_train)\n",
    "        performance_train[classifier_name].append(roc_auc_train)\n",
    "           \n",
    "        y_pred = classifier.predict(X_test)\n",
    "        #Test scores\n",
    "        f1_test = f1_score(y_test, y_pred)\n",
    "        accuracy_test = accuracy_score(y_test, y_pred)\n",
    "        precision_test = precision_score(y_test, y_pred)\n",
    "        recall_test = recall_score(y_test, y_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_pred)\n",
    "        #Print test scores          \n",
    "        print(f\"Test scores: \\nf1-score: {round(f1_test,3)}\\tAccuracy: {round(accuracy_test,3)}\\tPrecision: {round(precision_test,3)}\\tRecall: {round(recall_test,3)}\\tROC-AUC: {round(roc_auc_test,3)}\")\n",
    "          #Save test scores\n",
    "        performance_test[classifier_name].append(f1_test)\n",
    "        performance_test[classifier_name].append(accuracy_test)\n",
    "        performance_test[classifier_name].append(precision_test)\n",
    "        performance_test[classifier_name].append(recall_test)\n",
    "        performance_test[classifier_name].append(roc_auc_test)\n",
    "\n",
    "        print(\"\\n**********************************************************************\")\n",
    "    except ImportError:\n",
    "        print(\"Classifier \\\"\" + classifier_name + \"failed.\")\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def XGB(LR = 0.1, n_est = 1000, max_d = 5, min_c = 1, gm = 0, colsample = 0.8, subs = 1, lambd = 0, alpha = 0):\n",
    "   \n",
    "    xgb_model = XGBClassifier( learning_rate=LR, \n",
    "                                            n_estimators=n_est,\n",
    "                                            max_depth=max_d,\n",
    "                                            min_child_weight=min_c,                         \n",
    "                                            gamma=gm,\n",
    "                                            colsample_bytree=colsample,\n",
    "                                            subsample=subs,\n",
    "                                            objective ='binary:logistic',                   \n",
    "                                            reg_lambda=lambd,\n",
    "                                            reg_alpha=alpha,\n",
    "                                            scale_pos_weight = 1,\n",
    "                                            tree_method='gpu_hist',\n",
    "                                            seed=42)\n",
    "\n",
    "\n",
    "    xgb_model.fit(X_train,y_train)\n",
    "    y_pred_train = xgb_model.predict(X_train)\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "\n",
    "    print('performance over the training set: ' + str(f1_score(y_train, y_pred_train)))\n",
    "    print('performance over the test set: ' + str(f1_score(y_test, y_pred)) + '\\n')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    \n",
    "def XGB_tuning (LR = 0.1, n_est = 1000, max_d = 5, min_c = 1, gm = 0, colsample = 0.8, subs = 1,\n",
    "                lambd = 0, alpha = 0, param_test = {'learning_rate':[i/100.0 for i in range(5,20,2)]} ):\n",
    "    \n",
    "    gsearch = GridSearchCV(estimator = XGBClassifier( learning_rate=LR, \n",
    "                                                    n_estimators=n_est,\n",
    "                                                    max_depth=max_d,\n",
    "                                                    min_child_weight=min_c,                         \n",
    "                                                    gamma=gm,\n",
    "                                                    colsample_bytree=colsample,\n",
    "                                                    subsample = subs,\n",
    "                                                    objective ='binary:logistic',\n",
    "                                                    reg_lambda=lambd,\n",
    "                                                    reg_alpha=alpha,\n",
    "                                                    scale_pos_weight = 1,\n",
    "                                                    tree_method='gpu_hist',\n",
    "                                                    seed=42), \n",
    "                            param_grid = param_test, \n",
    "                            scoring='f1',\n",
    "                            n_jobs=4, \n",
    "                            cv=3)\n",
    "\n",
    "\n",
    "    gsearch.fit(X_train, y_train)\n",
    "    return gsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance over the training set: 0.949697675391195\n",
      "performance over the test set: 0.9323741772960316\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.90      0.89     34008\n",
      "         1.0       0.94      0.93      0.93     56752\n",
      "\n",
      "    accuracy                           0.92     90760\n",
      "   macro avg       0.91      0.91      0.91     90760\n",
      "weighted avg       0.92      0.92      0.92     90760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 1 - First XGB - A little bit overfitted, but could improve more the training set\n",
    "LearningRate = 0.1\n",
    "n_estimators = 1000\n",
    "\n",
    "# Fix Learning Rate and n_estimators\n",
    "XGB(LearningRate, n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2 - Tuning max_depth and min_child_weight\n",
    "\n",
    "parameters_test = {\n",
    "    'max_depth':range(3,7,2),\n",
    "    'min_child_weight':range(1,7,2)\n",
    "}\n",
    "\n",
    "gsearch2 = XGB_tuning (LR = LearningRate, n_est = n_estimators, param_test=parameters_test)\n",
    "best_max_depth, best_min_child_weight = gsearch2.best_params_['max_depth'], gsearch2.best_params_['min_child_weight']\n",
    "print(f\"{gsearch2.best_params_} CV_Score: {gsearch2.best_score_}\")\n",
    "\n",
    "# Evaluation\n",
    "XGB(LR = LearningRate, n_est = n_estimators, max_d=best_max_depth, min_c= best_min_child_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0.1} CV_Score: 0.9453991370659339\n",
      "performance over the training set: 0.953789735362048\n",
      "performance over the test set: 0.9263614290669442\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.85      0.87     34008\n",
      "         1.0       0.91      0.94      0.93     56752\n",
      "\n",
      "    accuracy                           0.91     90760\n",
      "   macro avg       0.90      0.89      0.90     90760\n",
      "weighted avg       0.91      0.91      0.91     90760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 3 - Tuning Gamma \n",
    "parameters_test = { \n",
    "    'gamma':[i/10.0 for i in range(0,5)] \n",
    "}\n",
    "\n",
    "gsearch3 = XGB_tuning (LR = LearningRate, n_est = n_estimators, max_d=best_max_depth, min_c= best_min_child_weight,\n",
    "                      param_test=parameters_test)\n",
    "best_gamma = gsearch3.best_params_['gamma']\n",
    "print(f\"{gsearch3.best_params_} CV_Score: {gsearch3.best_score_}\")\n",
    "\n",
    "# Evaluation\n",
    "XGB(LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, min_c = best_min_child_weight, gm = best_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.7, 'subsample': 0.9} CV_Score: 0.9456485859903755\n",
      "performance over the training set: 0.9538318465498397\n",
      "performance over the test set: 0.9256015040851159\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.85      0.87     34008\n",
      "         1.0       0.91      0.94      0.93     56752\n",
      "\n",
      "    accuracy                           0.91     90760\n",
      "   macro avg       0.90      0.89      0.90     90760\n",
      "weighted avg       0.91      0.91      0.90     90760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 4 - Tuning colsample_bytree and subsaample\n",
    "parameters_test = {\n",
    "    'colsample_bytree':[i/10.0 for i in range(7,11)],\n",
    "    'subsample':[i/10.0 for i in range(7,11)]\n",
    "}\n",
    "\n",
    "gsearch4 = XGB_tuning (LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, min_c = best_min_child_weight,\n",
    "                      gm = best_gamma, param_test=parameters_test)\n",
    "best_colsample_bytree, best_subsample = gsearch4.best_params_['colsample_bytree'], gsearch4.best_params_['subsample']\n",
    "print(f\"{gsearch4.best_params_} CV_Score: {gsearch4.best_score_}\")\n",
    "\n",
    "# Evaluation\n",
    "XGB(LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, \n",
    "                       min_c = best_min_child_weight, gm = best_gamma,  colsample = best_colsample_bytree, \n",
    "                       subs = best_subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_alpha': 0.5, 'reg_lambda': 0.5} CV_Score: 0.9454935526695749\n"
     ]
    }
   ],
   "source": [
    "# STEP 5 - Tuning Regularization Parameters\n",
    "# Lambda L2 Regularization\n",
    "# Ampha L1 Regularization\n",
    "\n",
    "parameters_test = {\n",
    "    'reg_lambda':[1e-2, 0.1, 0.5, 1, 2, 10],\n",
    "    'reg_alpha':[1e-2, 0.1, 0.5, 1, 2, 10],\n",
    "}\n",
    "\n",
    "gsearch5 = XGB_tuning (LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, \n",
    "                       min_c = best_min_child_weight, gm = best_gamma,  colsample = best_colsample_bytree, \n",
    "                       subs = best_subsample, param_test=parameters_test)\n",
    "\n",
    "best_reg_lambda, best_reg_alpha = gsearch5.best_params_['reg_lambda'], gsearch5.best_params_['reg_alpha']\n",
    "print(f\"{gsearch5.best_params_} CV_Score: {gsearch5.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reg_alpha': 0.4, 'reg_lambda': 0.4} CV_Score: 0.9454991058137155\n",
      "performance over the training set: 0.9532942998992946\n",
      "performance over the test set: 0.9257615475788801\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.84      0.87     34008\n",
      "         1.0       0.91      0.94      0.93     56752\n",
      "\n",
      "    accuracy                           0.91     90760\n",
      "   macro avg       0.90      0.89      0.90     90760\n",
      "weighted avg       0.91      0.91      0.90     90760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Closer Look\n",
    "parameters_test = {\n",
    "    'reg_lambda':[best_reg_lambda*0.8, best_reg_lambda, best_reg_lambda*1.2, best_reg_lambda*1.5],\n",
    "    'reg_alpha':[best_reg_alpha*0.8, best_reg_alpha, best_reg_alpha*1.2, best_reg_alpha*1.5]\n",
    "}\n",
    "\n",
    "gsearch5 = XGB_tuning (LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, min_c = best_min_child_weight,\n",
    "                       gm = best_gamma,  colsample = best_colsample_bytree, subs = best_subsample, \n",
    "                       param_test=parameters_test)\n",
    "\n",
    "best_reg_lambda, best_red_alpha = gsearch5.best_params_['reg_lambda'], gsearch5.best_params_['reg_alpha']\n",
    "print(f\"{gsearch5.best_params_} CV_Score: {gsearch5.best_score_}\")\n",
    "\n",
    "# Evaluation\n",
    "XGB(LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, min_c = best_min_child_weight,\n",
    "                       gm = best_gamma,  colsample = best_colsample_bytree, subs = best_subsample, \n",
    "                       lambd = best_reg_lambda, alpha = best_reg_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.03} CV_Score: 0.946415742107483\n",
      "performance over the training set: 0.9659243311922072\n",
      "performance over the test set: 0.9262006690122537\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.85      0.87     34008\n",
      "         1.0       0.91      0.94      0.93     56752\n",
      "\n",
      "    accuracy                           0.91     90760\n",
      "   macro avg       0.90      0.89      0.90     90760\n",
      "weighted avg       0.91      0.91      0.91     90760\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # STEP 6 - Reducing Learning Rate and Adding More Trees\n",
    "n_estimators = 10000\n",
    "parameters_test = {\n",
    "    'learning_rate':[i/100.0 for i in range(1,10)]\n",
    "}\n",
    "\n",
    "gsearch6 = XGB_tuning (LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, min_c = best_min_child_weight,\n",
    "                       gm = best_gamma,  colsample = best_colsample_bytree, subs = best_subsample, \n",
    "                       lambd = best_reg_lambda, alpha = best_reg_alpha, param_test=parameters_test)\n",
    "\n",
    "LearningRate = gsearch6.best_params_['learning_rate']\n",
    "print(f\"{gsearch6.best_params_} CV_Score: {gsearch6.best_score_}\")\n",
    "\n",
    "\n",
    "# Final Evaluation\n",
    "XGB(LR = LearningRate, n_est = n_estimators, max_d = best_max_depth, min_c = best_min_child_weight,\n",
    "                       gm = best_gamma,  colsample = best_colsample_bytree, subs = best_subsample, \n",
    "                       lambd = best_reg_lambda, alpha = best_reg_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Submission Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Testing File\n",
    "X_submission = pd.read_csv('../data/raw/testing.txt', header = None, sep = ' ', names = ['node', 'target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networkx Feature Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Networkx Features\n",
    "X, X_submission = nxGenerateFeatures(X, X_submission, y)\n",
    "\n",
    "# Save Processed Results\n",
    "SaveData('../data/processed/networkx/', X, X_submission, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Networkx Processed Data\n",
    "X, X_submission, y = file_io.LoadData('../data/processed/networkx/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### TF-IDF Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/similarity.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.dot(vec1, vec2.T)/(np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
      "../src/similarity.py:17: RuntimeWarning: invalid value encountered in arccos\n",
      "  return np.arccos(self.Cosine(vec1, vec2)) + np.radians(10)\n"
     ]
    }
   ],
   "source": [
    "# 2) TF-IDF Features\n",
    "X, X_submission = tfidfGenerateFeatures(X, X_submission)\n",
    "\n",
    "# Save tf-idf Intermediate Results\n",
    "file_io.SaveData('../data/processed/tf-idf/', X, X_submission, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TF-IDF Intermediate Data\n",
    "X, X_submission, y = file_io.LoadData('../data/processed/tf-idf/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Doc2Vec Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Doc2Vec Features\n",
    " X, X_submission = doc2vecGenerateFeatures(X, X_submission, loadPath=\"../results/models/doc2vec/vec100.model\", similarity=True)\n",
    "\n",
    "# Save Doc2Vec Intermediate Results\n",
    "file_io.SaveData('../data/processed/doc2vec/',  X, X_submission, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Doc2Vec Intermediate Data\n",
    "X, X_submission, y = file_io.LoadData('../data/processed/doc2vec/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "###  Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = { 'learning_rate': 0.04, \n",
    "                'n_estimators': 10000,\n",
    "                'max_depth': 5,\n",
    "                'min_child_weight': 1,                         \n",
    "                'gamma': 0.1,\n",
    "                'colsample_bytree': 1,\n",
    "                'subsample': 0.8,\n",
    "                'objective': 'binary:logistic',                   \n",
    "                'reg_lambda': 1,\n",
    "                'reg_alpha': 0.6,\n",
    "                'scale_pos_weight': 1,\n",
    "                'tree_method': 'gpu_hist',\n",
    "                'seed': 42}\n",
    "\n",
    "xgb_final_model = XGBClassifier(**param)\n",
    "\n",
    "\n",
    "xgb_final_model.fit(X,y)\n",
    "y_pred = xgb_final_model.predict(X_submission)\n",
    "y_pred = y_pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "result_file_name = '../results/predictions/tf-idf_predictions.csv'\n",
    "pd.DataFrame(y_pred, columns = ['predicted']).to_csv(result_file_name, sep=',', index=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113445</th>\n",
       "      <td>113445</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113446</th>\n",
       "      <td>113446</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113447</th>\n",
       "      <td>113447</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113448</th>\n",
       "      <td>113448</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113449</th>\n",
       "      <td>113449</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113450 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  predicted\n",
       "0            0          1\n",
       "1            1          1\n",
       "2            2          0\n",
       "3            3          0\n",
       "4            4          1\n",
       "...        ...        ...\n",
       "113445  113445          0\n",
       "113446  113446          0\n",
       "113447  113447          0\n",
       "113448  113448          1\n",
       "113449  113449          1\n",
       "\n",
       "[113450 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify Results\n",
    "submission = pd.read_csv(result_file_name, sep=',')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### REFERENCES ####\n",
    "# https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf\n",
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "# https://github.com/taki0112/Vector_Similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}