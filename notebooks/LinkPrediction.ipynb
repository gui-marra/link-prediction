{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install numpy \n",
    "%conda install pandas\n",
    "%conda install -c anaconda scikit-learn \n",
    "%conda install networkx\n",
    "%conda install -c conda-forge tpot\n",
    "%conda install -c conda-forge ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%conda install pytorch torchvision -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d35c46f8d1a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch'"
     ]
    }
   ],
   "source": [
    "import pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.read_csv('../data/raw/training.txt', header = None, sep = ' ', names = ['node', 'target', 'edge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = links[['node', 'target']]\n",
    "y = links['edge']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networkx Link Prediction Features\n",
    "\n",
    "1. Split the dataset \n",
    "2. Create graph connections with the training set \n",
    "3. Predict new coefficients for training and test set (feature generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateGraph (X, y):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(1,33227))\n",
    "    G.add_edges_from( list(X[y == 1][['node','target']].itertuples(index=False, name=None)) )\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AppendNextworkxFeature (function, G, X):\n",
    "    \n",
    "    column_name = str(function).split()[1]\n",
    "    \n",
    "    tuple_list = list(X[['node','target']].itertuples(index=False, name=None))\n",
    "    coef_generator = function(G, tuple_list)\n",
    "    coef_df = pd.DataFrame(coef_generator)\n",
    "    X.insert(2, column_name, list(coef_df[2]), allow_duplicates = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nxGenerateFeatures (X_train, X_test, y_train):\n",
    "    \n",
    "    # 2. Create Graph\n",
    "    # Total number of nodes=pages: 33.226\n",
    "    G = CreateGraph (X_train, y_train)\n",
    "    \n",
    "    # 3. Predict new coefficient/feature for defined link prediction function\n",
    "    linkPredictionFunctions = [nx.resource_allocation_index, nx.jaccard_coefficient, nx.adamic_adar_index, nx.preferential_attachment]\n",
    "    for function in linkPredictionFunctions:\n",
    "        AppendNextworkxFeature(function, G, X_train)\n",
    "        AppendNextworkxFeature(function, G, X_test)\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Stratified Split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2/3. Create Graph and generate features \n",
    "X_train, X_test = nxGenerateFeatures (X_train, X_test, y_train)\n",
    "\n",
    "# Save Intermediate Results\n",
    "X_train.to_csv('../data/intermediate/X_train_nx.csv', sep=',', index=False)\n",
    "X_test.to_csv('../data/intermediate/X_test_nx.csv', sep=',', index=False)\n",
    "y_train.to_csv('../data/intermediate/y_train_nx.csv', sep=',', index=False)\n",
    "y_test.to_csv('../data/intermediate/y_test_nx.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Networkx Intermediate Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/intermediate/X_train_nx.csv', sep=',')\n",
    "X_test = pd.read_csv('../data/intermediate/X_test_nx.csv', sep=',')\n",
    "\n",
    "y_train = np.ravel(pd.read_csv('../data/intermediate/y_train_nx.csv', sep=','))\n",
    "y_test = np.ravel(pd.read_csv('../data/intermediate/y_test_nx.csv', sep=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'G' not in locals():\n",
    "    G = CreateGraph (X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First classification model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "##Classifiers\n",
    "from sklearn.ensemble import AdaBoostClassifier #begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted\n",
    "from sklearn.ensemble import BaggingClassifier #Bagging classifier fits base classifiers each on random subsets of the original dataset and aggregate their individual predictions\n",
    "from sklearn.ensemble import ExtraTreesClassifier #Extremely Random Trees: This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting\n",
    "from sklearn.ensemble import GradientBoostingClassifier #GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier #Classifier implementing the k-nearest neighbors vote.\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scale(X):\n",
    "    scaled_features = StandardScaler().fit_transform(X.values)\n",
    "    return pd.DataFrame(scaled_features, index = X.index, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation parameters\n",
    "'''Better performance withou scaling'''\n",
    "scale = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "if 'node' and 'target' in X_train.columns:\n",
    "    X_train = X_train.drop(columns = ['node', 'target'])\n",
    "if 'node' and 'target' in X_test.columns:\n",
    "    X_test = X_test.drop(columns = ['node', 'target'])\n",
    "    \n",
    "\n",
    "if scale:  \n",
    "    X_train = Scale(X_train)\n",
    "    X_test = Scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Models\n",
    "print(\"Summary for classifiers:\")\n",
    "\n",
    "clf = [\n",
    "            [AdaBoostClassifier(), \"AdaBoostClassifier\"],\n",
    "            [BaggingClassifier(), \"BaggingClassifier\"],\n",
    "            [ExtraTreesClassifier(), \"ExtraTreesClassifier\"],\n",
    "            [GradientBoostingClassifier(), \"GradientBoostClassifier\"],\n",
    "            [DecisionTreeClassifier(), \"DecisionTreeClassifier\"],\n",
    "            [RandomForestClassifier(), \"RandomForestClassifier\"]\n",
    "        ]\n",
    "\n",
    "performance_train = {}\n",
    "performance_test = {}\n",
    "    \n",
    "for classifier, clf_name in clf: performance_train[clf_name] = []\n",
    "for classifier, clf_name in clf: performance_test[clf_name] = []\n",
    "\n",
    "    \n",
    "for elem in clf: #Use each classifier in clf\n",
    "    classifier = elem[0]\n",
    "    classifier_name = elem[1]\n",
    "    print(classifier_name)\n",
    "        \n",
    "    try:    \n",
    "        classifier.fit(X_train, y_train)\n",
    "            \n",
    "        y_hat = classifier.predict(X_train)\n",
    "        #Train Scores:\n",
    "        f1_train = f1_score(y_train, y_hat)\n",
    "        accuracy_train = accuracy_score(y_train, y_hat)\n",
    "        precision_train = precision_score(y_train, y_hat)\n",
    "        recall_train = recall_score(y_train, y_hat)\n",
    "        roc_auc_train = roc_auc_score(y_train, y_hat)\n",
    "        #Print train Scores\n",
    "        print(f\"Train scores: \\nf1-score: {round(f1_train,3)}\\tAccuracy: {round(accuracy_train, 3)}\\tPrecision: {round(precision_train,3)}\\tRecall: {round(recall_train,3)}\\tROC-AUC: {round(roc_auc_train,3)}\")\n",
    "        #Sava train scors for comparison\n",
    "        performance_train[classifier_name].append(f1_train)\n",
    "        performance_train[classifier_name].append(accuracy_train)\n",
    "        performance_train[classifier_name].append(precision_train)\n",
    "        performance_train[classifier_name].append(recall_train)\n",
    "        performance_train[classifier_name].append(roc_auc_train)\n",
    "           \n",
    "        y_pred = classifier.predict(X_test)\n",
    "        #Test scores\n",
    "        f1_test = f1_score(y_test, y_pred)\n",
    "        accuracy_test = accuracy_score(y_test, y_pred)\n",
    "        precision_test = precision_score(y_test, y_pred)\n",
    "        recall_test = recall_score(y_test, y_pred)\n",
    "        roc_auc_test = roc_auc_score(y_test, y_pred)\n",
    "        #Print test scores          \n",
    "        print(f\"Test scores: \\nf1-score: {round(f1_test,3)}\\tAccuracy: {round(accuracy_test,3)}\\tPrecision: {round(precision_test,3)}\\tRecall: {round(recall_test,3)}\\tROC-AUC: {round(roc_auc_test,3)}\")\n",
    "          #Save test scores\n",
    "        performance_test[classifier_name].append(f1_test)\n",
    "        performance_test[classifier_name].append(accuracy_test)\n",
    "        performance_test[classifier_name].append(precision_test)\n",
    "        performance_test[classifier_name].append(recall_test)\n",
    "        performance_test[classifier_name].append(roc_auc_test)\n",
    "\n",
    "        print(\"\\n**********************************************************************\")\n",
    "    except ImportError:\n",
    "        print(\"Classifier \\\"\" + classifier_name + \"failed.\")\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tpot auto-ml tool for hyper-parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "if 'node' and 'target' in X_train.columns:\n",
    "    X_train = X_train.drop(columns = ['node', 'target'])\n",
    "if 'node' and 'target' in X_test.columns:\n",
    "    X_test = X_test.drop(columns = ['node', 'target'])\n",
    "    \n",
    "\n",
    "tpot = TPOTClassifier(generations = 5, population_size = 20, cv = 3, verbosity=2, scoring = 'f1')\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "tpot.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing File\n",
    "X_submission = pd.read_csv('../data/raw/testing.txt', header = None, sep = ' ', names = ['node', 'target'])\n",
    "\n",
    "# Feature Generation\n",
    "X, X_submission = nxGenerateFeatures(X, X_submission, y)\n",
    "\n",
    "# Save Processed Results\n",
    "X.to_csv('../data/processed/X_nx.csv', sep=',', index=False)\n",
    "y.to_csv('../data/processed/y_nx.csv', sep=',', index=False)\n",
    "X_submission.to_csv('../data/processed/X_submission.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Processed Results Networkx\n",
    "X = pd.read_csv('../data/processed/X_nx.csv', sep = ',')\n",
    "y = pd.read_csv('../data/processed/y_nx.csv', sep = ',')\n",
    "X_submission = pd.read_csv('../data/processed/X_submission_nx.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "# could be 'or' since these columns are always removed together\n",
    "if 'node' and 'target' in X.columns:\n",
    "    X = X.drop(columns = ['node', 'target'])\n",
    "    \n",
    "if 'node' and 'target' in X_submission.columns:\n",
    "    X_submission = X_submission.drop(columns = ['node', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction according to Ada Boost Clssifier  \n",
    "\n",
    "elem = [AdaBoostClassifier(), \"AdaBoostClassifier\"]\n",
    "classifier = elem[0]\n",
    "classifier_name = elem[1]\n",
    "print(classifier_name)\n",
    "\n",
    "try:    \n",
    "    classifier.fit(X, y)\n",
    "    y_hat = classifier.predict(X)\n",
    "        \n",
    "    #Train Scores:\n",
    "    f1_train = f1_score(y, y_hat)\n",
    "    accuracy_train = accuracy_score(y, y_hat)\n",
    "    precision_train = precision_score(y, y_hat)\n",
    "    recall_train = recall_score(y, y_hat)\n",
    "    roc_auc_train = roc_auc_score(y, y_hat)\n",
    "    #Print train Scores\n",
    "    print(f\"Train scores: \\nf1-score: {round(f1_train,3)}\\tAccuracy: {round(accuracy_train, 3)}\\tPrecision: {round(precision_train,3)}\\tRecall: {round(recall_train,3)}\\tROC-AUC: {round(roc_auc_train,3)}\")\n",
    "    \n",
    "    y_pred = classifier.predict(X_submission)\n",
    "    print(\"\\n**********************************************************************\")\n",
    "except ImportError:\n",
    "    print(\"Classifier \\\"\" + classifier_name + \"failed.\")\n",
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "pd.DataFrame(y_pred, columns = ['predicted']).to_csv('../results/outputs/nxAdaBoost.csv', sep=',', index=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../results/outputs/nxAdaBoost.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
